{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.kaggle.com/yekenot/pooled-gru-fasttext/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Dropout, TimeDistributed\n",
    "from keras.layers import GRU, LSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, Multiply\n",
    "from keras.layers import RepeatVector, Activation, Lambda, Average\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback, CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = 'fasttext' #'glove', 'fasttext\n",
    "\n",
    "if embeddings == 'fasttext':\n",
    "    EMBEDDING_FILE = '../data/fasttext/crawl-300d-2M.vec'\n",
    "else:\n",
    "    EMBEDDING_FILE = '../data/glove/glove.840B.300d.txt'    \n",
    "\n",
    "max_features = 100000  #100000 , 30000\n",
    "maxlen = 200\n",
    "embed_size = 300\n",
    "prefix = 'c1' #x, #c1\n",
    "\n",
    "print(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(word_index), max_features\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_feats_path = '../models/{}_train_feat_{}_seq_{}.pkl'.format(prefix, max_features, maxlen)\n",
    "test_feats_path = '../models/{}_test_feat_{}_seq_{}.pkl'.format(prefix, max_features, maxlen)\n",
    "embedding_matrix_path = '../models/{}_{}_embedding_matrix_feat_{}.pkl'.format(prefix, embeddings, max_features)\n",
    "print(train_feats_path)\n",
    "\n",
    "#pickle.dump(x_train, open(train_feats_path, 'wb'))\n",
    "#pickle.dump(x_test, open(test_feats_path, 'wb'))\n",
    "#pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n",
    "\n",
    "x_train = pickle.load(open(train_feats_path, 'rb') )\n",
    "x_test = pickle.load(open(test_feats_path, 'rb') )\n",
    "embedding_matrix = pickle.load(open(embedding_matrix_path, 'rb') )\n",
    "\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = 0        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "            # stopping condition - ROC stops improving\n",
    "            if score > self.best:\n",
    "                self.best = score\n",
    "            else:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))            \n",
    "\n",
    "def get_model(): # base\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.0, dropout=0.2))(x)\n",
    "    x = TimeDistributed(Dense(100, activation = \"relu\"))(x) # time distributed  (sigmoid)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    # global pooling layer\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_2(): # 2 dense final layers\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.0, dropout=0.2))(x)\n",
    "    x = TimeDistributed(Dense(100, activation = \"relu\"))(x) # time distributed  (sigmoid)\n",
    "    x = Dropout(0.2)(x)    \n",
    "    #x = TimeDistributed(Dense(100, activation = \"relu\"))(x) # time distributed  (sigmoid)\n",
    "    #x = Dropout(0.2)(x)\n",
    "    \n",
    "    # global pooling layer\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    # dense layers\n",
    "    outp = Dense(64, activation=\"relu\")(conc)\n",
    "    outp = Dropout(0.1)(outp)    \n",
    "    outp = Dense(6, activation=\"sigmoid\")(outp)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_3(): #regularized\n",
    "    kern_reg = regularizers.l2(0.00001)\n",
    "    bias_reg = regularizers.l2(0.00001)    \n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.0, dropout=0.2, kernel_regularizer=kern_reg, bias_regularizer=bias_reg))(x)\n",
    "    x = TimeDistributed(Dense(100, activation = \"relu\"))(x) # time distributed  (sigmoid)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    # global pooling layer\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_4(): # 2 LSTMs separately pooled\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "\n",
    "    x1 = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.0, dropout=0.2))(x)    \n",
    "    x2 = Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.0, dropout=0.2))(x)\n",
    "    conc = concatenate([x1, x2])\n",
    "    \n",
    "    #x = TimeDistributed(Dense(100, activation = \"relu\"))(x) # time distributed  (sigmoid)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    \n",
    "    # global pooling layer\n",
    "    avg_pool = GlobalAveragePooling1D()(conc)\n",
    "    max_pool = GlobalMaxPooling1D()(conc)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    x = Dense(64, activation='relu')(conc)\n",
    "    x = Dropout(0.1)(x)    \n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model_3()\n",
    "#opt = Adam(lr=0.005, decay=0.01, beta_1=0.9, beta_2=0.999)\n",
    "opt = Nadam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(maxlen)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation('softmax', name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)\n",
    "\n",
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combination of a single LSTM output and all LSTM state pooling combination \n",
    "def get_model_attention(Ty, n_a, n_s): #is all you need attention ?\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')    \n",
    "    s = s0\n",
    "    c = c0\n",
    "\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.0, dropout=0.1))(x)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(x, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
    "        \n",
    "    # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "    out1 = Dense(64, activation=\"relu\")(s)\n",
    "    out1 = Dropout(0.1)(out1)\n",
    "        \n",
    "    # global pooling layer\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    out2 = Dense(64, activation=\"relu\")(conc)\n",
    "    out2 = Dropout(0.1)(out2)\n",
    "    \n",
    "    conc2 = concatenate([out1, out2])\n",
    "    output = Dense(6, activation=\"sigmoid\")(conc2)\n",
    "    \n",
    "    model = Model(inputs=[inp, s0, c0] , outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dense layers for output\n",
    "\n",
    "def get_model_attention_2(Ty, n_a, n_s): #is all you need attention ?\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')    \n",
    "    s = s0\n",
    "    c = c0\n",
    "    outputs = []\n",
    "\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.0, dropout=0.1))(x)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(x, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
    "        \n",
    "        output = Dense(6, activation=\"relu\")(s)\n",
    "        output = Dropout(0.1)(output)\n",
    "        outputs.append(output)                \n",
    "    \n",
    "    conc = concatenate(outputs)\n",
    "    out = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp, s0, c0] , outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using average for output\n",
    "def get_model_attention_3(Ty, n_a, n_s): #is all you need attention ?\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')    \n",
    "    s = s0\n",
    "    c = c0\n",
    "    outputs = []\n",
    "\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.0, dropout=0.1))(x)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(x, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
    "        \n",
    "        output = Dense(64, activation=\"relu\")(s)\n",
    "        output = Dropout(0.1)(output)\n",
    "        outputs.append(output)                \n",
    "\n",
    "    avg = Average()(outputs)\n",
    "    out = Dense(6, activation=\"sigmoid\")(avg)\n",
    "    \n",
    "    model = Model(inputs=[inp, s0, c0] , outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_attention_3(maxlen, n_a, n_s)\n",
    "#opt = Nadam(lr=0.001)\n",
    "opt = Adam(lr=0.005, decay=0.01, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning & inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "CheckPoint = ModelCheckpoint('../snapshots/weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "csv_logger = CSVLogger('../training.log')\n",
    "early_stop = EarlyStopping(patience=1, verbose=2)\n",
    "\n",
    "#X_tra.shape, x_train.shape\n",
    "X_tra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular models\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc, csv_logger, CheckPoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention models\n",
    "s0 = np.zeros((X_tra.shape[0], n_s))\n",
    "c0 = np.zeros((X_tra.shape[0], n_s))\n",
    "\n",
    "s0_val = np.zeros((X_val.shape[0], n_s))\n",
    "c0_val = np.zeros((X_val.shape[0], n_s))\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=([X_val, np.copy(s0_val), np.copy(c0_val)], y_val), interval=1)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "hist = model.fit([X_tra, s0, c0], y_tra, epochs=epochs, batch_size=batch_size, validation_data=([X_val, s0_val, c0_val], y_val),\n",
    "                 callbacks=[csv_logger, RocAuc, CheckPoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((x_test.shape[0], n_s))\n",
    "c0 = np.zeros((x_test.shape[0], n_s))\n",
    "\n",
    "y_pred = model.predict([x_test, s0, c0], batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified k-fold learning & inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Stratified k-fold training\n",
    "\n",
    "n_folds = 5\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "predict_batch_size = 1024\n",
    "run_id = 'gru_reg_fasttext_128'\n",
    "opt = Nadam(lr=0.001) #optimizer\n",
    "#opt = Adam(lr=0.003, decay=0.01, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = 20, shuffle = True, random_state = 32)\n",
    "\n",
    "csv_logger = CSVLogger('../training.log', append=True)\n",
    "early_stop = EarlyStopping(verbose=2)\n",
    "\n",
    "pred = np.zeros((x_test.shape[0], 6))\n",
    "y_packed = np.packbits(y_train, axis=1)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold.split(x_train, y_packed)):\n",
    "    print(\"Running fold {} / {}\".format(i + 1, n_folds))\n",
    "    print(\"Training / Valid set counts {} / {}\".format(train_idx.shape, valid_idx.shape))\n",
    "\n",
    "    model = None    \n",
    "    model = get_model_3()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    \n",
    "    xs_train, ys_train = x_train[train_idx], y_train[train_idx] \n",
    "    xs_valid, ys_valid = x_train[valid_idx], y_train[valid_idx]\n",
    "\n",
    "    CheckPoint = ModelCheckpoint('../snapshots/' + run_id + '_fold_' + str(i) + '_weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "    RocAuc = RocAucEvaluation(validation_data=(xs_valid, ys_valid), interval=1)\n",
    "\n",
    "    # training\n",
    "    history = model.fit(xs_train, ys_train, batch_size = batch_size, epochs = epochs, validation_data = (xs_valid, ys_valid), \n",
    "                          verbose = 2, callbacks=[RocAuc, csv_logger, CheckPoint])        \n",
    "    # predict\n",
    "    pred += model.predict(x_test, batch_size = predict_batch_size, verbose = 1)\n",
    "\n",
    "    if (i + 1) == n_folds: break    \n",
    "    \n",
    "y_pred = pred/n_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading and predicting\n",
    "\n",
    "predict_batch_size = 1024\n",
    "model_folder = 'gru_fasttext_128_95p_reg/'\n",
    "models_files = ['gru_reg_fasttext_128_fold_0_weights.04-0.04.hdf5', 'gru_reg_fasttext_128_fold_1_weights.02-0.04.hdf5', \n",
    "               'gru_reg_fasttext_128_fold_2_weights.03-0.04.hdf5', 'gru_reg_fasttext_128_fold_3_weights.02-0.04.hdf5',\n",
    "               'gru_reg_fasttext_128_fold_4_weights.02-0.04.hdf5']\n",
    "scores = [0.989979, 0.988774, 0.991019, 0.989545, 0.989145]\n",
    "\n",
    "\n",
    "preds = [load_model('../snapshots/'+model_folder+f).predict(x_test, batch_size = predict_batch_size, verbose = 1) for f in models_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions - equal averaging\n",
    "\n",
    "y_pred = np.array(preds).mean(axis=0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted average of predictions according to the score of each fold\n",
    "\n",
    "score_rank = np.array(scores).argsort().argsort()\n",
    "print(score_rank+1)\n",
    "y_pred = np.average(preds, axis=0, weights=score_rank+1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual model prediction\n",
    "\n",
    "model_name = 'weights.02-0.04.hdf5'\n",
    "model = load_model('../snapshots/' + model_name)\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "outfile = 'gru_fasttext_128_95p_reg.5fold_weighted_avg.csv'\n",
    "submission.to_csv('../submissions/' + outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "### standard LSTM + FastText + TimeDistributed(tanh activation)\n",
    "\n",
    "Running fold 1 / 5\n",
    "Training / Valid set counts (143603,) / (15968,)\n",
    "Train on 143603 samples, validate on 15968 samples\n",
    "Epoch 1/20\n",
    " - 396s - loss: 0.0757 - acc: 0.9755 - val_loss: 0.0468 - val_acc: 0.9825\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.977484 \n",
    "\n",
    "Epoch 2/20\n",
    " - 391s - loss: 0.0451 - acc: 0.9833 - val_loss: 0.0449 - val_acc: 0.9832\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.980896 \n",
    "\n",
    "Epoch 3/20\n",
    " - 390s - loss: 0.0401 - acc: 0.9845 - val_loss: 0.0436 - val_acc: 0.9833\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.983498 \n",
    "\n",
    "Epoch 4/20\n",
    " - 391s - loss: 0.0365 - acc: 0.9857 - val_loss: 0.0427 - val_acc: 0.9836\n",
    "\n",
    " ROC-AUC - epoch: 4 - score: 0.984817 \n",
    "\n",
    "Epoch 5/20\n",
    " - 391s - loss: 0.0334 - acc: 0.9868 - val_loss: 0.0438 - val_acc: 0.9831\n",
    "\n",
    " ROC-AUC - epoch: 5 - score: 0.986026 \n",
    "\n",
    "#### LSTM (maxlen 200, Units 64) + FastText + MaxPool, LSTM - dropout=0.2, recurrent_dropout=0.0, Spatial Dropout 0.4, TimeDistributed (100, relu, dr 0.2), final dense 64 - dr 0.1 - Batch size 256, - Ep5, Max features 100,000  - With cleaned text\n",
    "\n",
    "Total params: 30,200,986\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/10\n",
    " - 402s - loss: 0.0653 - acc: 0.9786 - val_loss: 0.0482 - val_acc: 0.9812\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.984191 \n",
    "\n",
    "Epoch 2/10\n",
    " - 398s - loss: 0.0410 - acc: 0.9841 - val_loss: 0.0440 - val_acc: 0.9827\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.988491 \n",
    "\n",
    "Epoch 3/10\n",
    " - 398s - loss: 0.0368 - acc: 0.9855 - val_loss: 0.0429 - val_acc: 0.9827\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.990585 \n",
    "\n",
    "Epoch 4/10\n",
    " - 398s - loss: 0.0335 - acc: 0.9867 - val_loss: 0.0418 - val_acc: 0.9837\n",
    "\n",
    " ROC-AUC - epoch: 4 - score: 0.990823 \n",
    "\n",
    "\n",
    "#### Best out of stratified - LSTM (maxlen 200, Units 64) + GloVe + MaxPool, LSTM - dropout=0.2, recurrent_dropout=0.0, Spatial Dropout 0.4, TimeDistributed (100, relu, dr 0.1), final dense 64 - dr 0.1 - Batch size 256, - Ep3, nadam 0.002, Max features 100,000 \n",
    "\n",
    "Train on 151582 samples, validate on 7989 samples\n",
    "Epoch 1/10\n",
    " - 409s - loss: 0.0579 - acc: 0.9795 - val_loss: 0.0447 - val_acc: 0.9832\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.984229 \n",
    "\n",
    "Epoch 2/10\n",
    " - 405s - loss: 0.0407 - acc: 0.9841 - val_loss: 0.0415 - val_acc: 0.9831\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.988578 \n",
    "\n",
    "Epoch 3/10\n",
    " - 405s - loss: 0.0357 - acc: 0.9857 - val_loss: 0.0443 - val_acc: 0.9826\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.989446 \n",
    "\n",
    "#### LSTM (maxlen 200, Units 64) + GloVe + MaxPool, LSTM - dropout=0.2, recurrent_dropout=0.0, Spatial Dropout 0.4, TimeDistributed (100, relu, dr 0.1), final dense 64 - dr 0.1 - Batch size 256, - Ep5, Max features 100,000 \n",
    "\n",
    "Total params: 30,213,034\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/5\n",
    " - 408s - loss: 0.0664 - acc: 0.9768 - val_loss: 0.0516 - val_acc: 0.9804\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.982877 \n",
    "\n",
    "Epoch 2/5\n",
    " - 405s - loss: 0.0434 - acc: 0.9833 - val_loss: 0.0450 - val_acc: 0.9819\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.988467 \n",
    "\n",
    "Epoch 3/5\n",
    " - 404s - loss: 0.0391 - acc: 0.9846 - val_loss: 0.0424 - val_acc: 0.9830\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.990159 \n",
    "\n",
    "Epoch 4/5\n",
    " - 402s - loss: 0.0358 - acc: 0.9856 - val_loss: 0.0412 - val_acc: 0.9834\n",
    "\n",
    " ROC-AUC - epoch: 4 - score: 0.990784 \n",
    "\n",
    "Epoch 5/5\n",
    " - 403s - loss: 0.0329 - acc: 0.9866 - val_loss: 0.0416 - val_acc: 0.9834\n",
    "\n",
    " ROC-AUC - epoch: 5 - score: 0.990300 \n",
    "\n",
    "Epoch 00005: early stopping\n",
    "\n",
    "**LB 0.9832**\n",
    "\n",
    "#### LSTM (maxlen 200, Units 64) + GloVe + MaxPool, LSTM - dropout=0.2, recurrent_dropout=0.0, Spatial Dropout 0.4, TimeDistributed (100, relu, dr 0.1), final dense 64 - dr 0.1 - Batch size 256, - Ep5, Max features 100,000 \n",
    "\n",
    "Total params: 30,213,034\n",
    "\n",
    "Name: **lstm_l1_64_spatial_dr_0_4_lstm_dr_0_2_timedistributed_dense_relu_100_dr_0_1_amaxpool_dense_64_dr_0_1_glove_ep6_batch_256_nadam_001.csv**\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/6\n",
    " - 416s - loss: 0.0637 - acc: 0.9785 - val_loss: 0.0473 - val_acc: 0.9819\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.984785 \n",
    "\n",
    "Epoch 2/6\n",
    " - 397s - loss: 0.0427 - acc: 0.9836 - val_loss: 0.0431 - val_acc: 0.9830\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989311 \n",
    "\n",
    "Epoch 3/6\n",
    " - 398s - loss: 0.0387 - acc: 0.9846 - val_loss: 0.0412 - val_acc: 0.9838\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.990382 \n",
    "\n",
    "Epoch 4/6\n",
    " - 399s - loss: 0.0354 - acc: 0.9858 - val_loss: 0.0420 - val_acc: 0.9829\n",
    "\n",
    " ROC-AUC - epoch: 4 - score: 0.990670 \n",
    "\n",
    "Epoch 5/6\n",
    " - 399s - loss: 0.0326 - acc: 0.9866 - val_loss: 0.0418 - val_acc: 0.9840\n",
    "\n",
    " ROC-AUC - epoch: 5 - score: 0.990223 \n",
    "\n",
    "Epoch 6/6\n",
    " - 400s - loss: 0.0302 - acc: 0.9876 - val_loss: 0.0433 - val_acc: 0.9838\n",
    "\n",
    " ROC-AUC - epoch: 6 - score: 0.989657 \n",
    "\n",
    "**LB 0.9843**\n",
    "\n",
    "#### LSTM (maxlen 200, Units 64) + Glove + MaxPool, LSTM - dropout=0.2, recurrent_dropout=0.0, Spatial Dropout 0.4, TimeDistributed (100, relu, dr 0.1) - Batch size 256, - Ep5, Max features 100,000 \n",
    "\n",
    "Total params: 30,200,986, Train on 151592 samples, validate on 7979 samples\n",
    "Name: **lstm_l1_64_spatial_dr_0_4_lstm_dr_0_2_timedistributed_dense_relu_100_glove_ep5_batch_256_nadam_001**\n",
    "\n",
    "Epoch 1/5\n",
    " - 420s - loss: 0.0662 - acc: 0.9778 - val_loss: 0.0469 - val_acc: 0.9824\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.982196 \n",
    "\n",
    "Epoch 2/5\n",
    " - 413s - loss: 0.0424 - acc: 0.9837 - val_loss: 0.0430 - val_acc: 0.9831\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.987828 \n",
    "\n",
    "Epoch 3/5\n",
    " - 410s - loss: 0.0382 - acc: 0.9850 - val_loss: 0.0411 - val_acc: 0.9840\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.989994 \n",
    "\n",
    "Epoch 4/5\n",
    " - 410s - loss: 0.0352 - acc: 0.9861 - val_loss: 0.0409 - val_acc: 0.9839\n",
    "\n",
    " ROC-AUC - epoch: 4 - score: 0.990150 \n",
    "\n",
    "Epoch 5/5\n",
    " - 409s - loss: 0.0324 - acc: 0.9871 - val_loss: 0.0417 - val_acc: 0.9836\n",
    "\n",
    " ROC-AUC - epoch: 5 - score: 0.990166\n",
    "\n",
    "\n",
    "#### LSTM (maxlen 200, Units 64) + Glove + MaxPool, LSTM - dropout=0.5, recurrent_dropout=0.0 - Spatial Dropout 0.4, TimeDistributed (100, relu) - Batch size 128, - Ep5, Max features 100,000 \n",
    "\n",
    "* Total params: 30,200,986\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/5\n",
    " - 806s - loss: 0.0571 - acc: 0.9797 - val_loss: 0.0450 - val_acc: 0.9825\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.984705 \n",
    "\n",
    "Epoch 2/5\n",
    " - 803s - loss: 0.0406 - acc: 0.9843 - val_loss: 0.0427 - val_acc: 0.9827\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989365 \n",
    "\n",
    "Epoch 3/5\n",
    " - 799s - loss: 0.0365 - acc: 0.9856 - val_loss: 0.0408 - val_acc: 0.9840\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.990350 \n",
    "\n",
    "Epoch 4/5\n",
    " - 799s - loss: 0.0330 - acc: 0.9868 - val_loss: 0.0425 - val_acc: 0.9824\n",
    "\n",
    " ROC-AUC - epoch: 4 - score: 0.990180 \n",
    "\n",
    "Epoch 5/5\n",
    " - 801s - loss: 0.0299 - acc: 0.9879 - val_loss: 0.0423 - val_acc: 0.9837\n",
    "\n",
    " ROC-AUC - epoch: 5 - score: 0.989926 \n",
    "\n",
    "\n",
    "#### LSTM (maxlen 200, Units 256) + FastText + MaxPool - Ep2 - dropout=0.5, recurrent_dropout=0.5 - Spatial Dropout 0.4 - Batch size 128, Max features 100,000\n",
    "\n",
    "* **Name: lstm_l1_256_spatial_dr_0_4_lstm_dr_0_5_glove_maxpool_ep3_batch_128_nadam.csv**\n",
    "* Total params: 31,146,886\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/2\n",
    " - 932s - loss: 0.0507 - acc: 0.9813 - val_loss: 0.0585 - val_acc: 0.9818\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.987029 \n",
    "\n",
    "Epoch 2/2\n",
    " - 917s - loss: 0.0391 - acc: 0.9847 - val_loss: 0.0505 - val_acc: 0.9822\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.988892 \n",
    "\n",
    "#### LSTM (maxlen 200, Units 256) + FastText + MaxPool - Ep3 - dropout=0.5, recurrent_dropout=0.5 - Spatial Dropout 0.4 - Batch size 256, Max features 100,000\n",
    "\n",
    "* Train on 151592 samples, validate on 7979 samples\n",
    "* Epoch 1/3\n",
    " - 640s - loss: 0.0539 - acc: 0.9802 - val_loss: 0.0602 - val_acc: 0.9822\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.988412 \n",
    "\n",
    "* Epoch 2/3\n",
    " - 635s - loss: 0.0390 - acc: 0.9847 - val_loss: 0.0533 - val_acc: 0.9815\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989449 \n",
    "\n",
    "* Epoch 3/3\n",
    " - 635s - loss: 0.0337 - acc: 0.9865 - val_loss: 0.0488 - val_acc: 0.9826\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.988610 \n",
    " \n",
    "* **LB 0.9824**\n",
    "\n",
    "\n",
    "#### LSTM (maxlen 200, Units 256) + GloVe + MaxPool - Ep3 - dropout=0.5, recurrent_dropout=0.5 - Spatial Dropout 0.4 - Batch size 256, Max features 100,000\n",
    "\n",
    "Total params: 31,146,886\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/3\n",
    " - 921s - loss: 0.0500 - acc: 0.9815 - val_loss: 0.0584 - val_acc: 0.9821\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.987247 \n",
    "\n",
    "Epoch 2/3\n",
    " - 918s - loss: 0.0383 - acc: 0.9849 - val_loss: 0.0500 - val_acc: 0.9824\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989146 \n",
    "\n",
    "Epoch 3/3\n",
    " - 917s - loss: 0.0332 - acc: 0.9867 - val_loss: 0.0483 - val_acc: 0.9818\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.988098 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
