{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.kaggle.com/umbertogriffo/combined-gru-and-cnn-fasttext-badwords/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import h5py\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Dropout\n",
    "from keras.layers import GRU, LSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback, CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/glove/glove.840B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "embeddings = 'glove' #'glove', 'fasttext\n",
    "\n",
    "if embeddings == 'fasttext':\n",
    "    EMBEDDING_FILE = '../data/fasttext/crawl-300d-2M.vec'\n",
    "else:\n",
    "    EMBEDDING_FILE = '../data/glove/glove.840B.300d.txt'    \n",
    "\n",
    "max_features = 100000  #100000 , 30000\n",
    "maxlen = 200\n",
    "embed_size = 300\n",
    "prefix = 'c1' #x, #c1\n",
    "\n",
    "print(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(word_index), max_features\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_feats_path = '../models/{}_train_feat_{}_seq_{}.pkl'.format(prefix, max_features, maxlen)\n",
    "test_feats_path = '../models/{}_test_feat_{}_seq_{}.pkl'.format(prefix, max_features, maxlen)\n",
    "embedding_matrix_path = '../models/{}_{}_embedding_matrix_feat_{}.pkl'.format(prefix, embeddings, max_features)\n",
    "\n",
    "#pickle.dump(x_train, open(train_feats_path, 'wb'))\n",
    "#pickle.dump(x_test, open(test_feats_path, 'wb'))\n",
    "#pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n",
    "\n",
    "x_train = pickle.load(open(train_feats_path, 'rb') )\n",
    "x_test = pickle.load(open(test_feats_path, 'rb') )\n",
    "embedding_matrix = pickle.load(open(embedding_matrix_path, 'rb') )\n",
    "\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "            # stopping condition - ROC stops improving\n",
    "            if score > self.best:\n",
    "                self.best = score\n",
    "            else:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
    "                \n",
    "\n",
    "def get_model():\n",
    "    input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(input)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Bidirectional(LSTM(80, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "\n",
    "    # http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/\n",
    "    # For text, CNN -> LSTM (or GRU) doesn't seem to work well, but LSTM -> CNN works really well.\n",
    "    x1 = Conv1D(filters=64, kernel_size=2, padding='valid', kernel_initializer=\"he_uniform\")(x)\n",
    "    x1 = Dropout(0.2)(x1)\n",
    "    \n",
    "    x2 = Conv1D(filters=64, kernel_size=3, padding='valid', kernel_initializer=\"he_uniform\")(x)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "    \n",
    "    # Global average pooling operation for temporal data.\n",
    "    # https://www.quora.com/What-is-global-average-pooling\n",
    "    avg_pool0 = GlobalAveragePooling1D()(x)\n",
    "    # Global max pooling operation for temporal data.\n",
    "    max_pool0 = GlobalMaxPooling1D()(x)    \n",
    "\n",
    "    # Global average pooling operation for temporal data.\n",
    "    # https://www.quora.com/What-is-global-average-pooling\n",
    "    avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "    # Global max pooling operation for temporal data.\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    # Global average pooling operation for temporal data.\n",
    "    # https://www.quora.com/What-is-global-average-pooling\n",
    "    avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "    # Global max pooling operation for temporal data.\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    \n",
    "    conc = concatenate([avg_pool0, max_pool0, avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "\n",
    "    output = Dense(64, activation=\"relu\")(conc)\n",
    "    output = Dropout(0.2)(output)\n",
    "    \n",
    "    output = Dense(6, activation=\"sigmoid\")(output)\n",
    "        \n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     30000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 200, 300)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200, 160)     243840      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 199, 64)      20544       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 198, 64)      30784       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 199, 64)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 198, 64)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 160)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 160)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 64)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 576)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           36928       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            390         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,332,486\n",
      "Trainable params: 30,332,486\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "opt = Nadam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # 32\n",
    "epochs = 3\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "CheckPoint = ModelCheckpoint('../snapshots/cnn_weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "csv_logger = CSVLogger('../training.log')\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc, csv_logger, CheckPoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified k-fold learning & inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1 / 4\n",
      "Training / Valid set counts (151582,) / (7989,)\n",
      "Train on 151582 samples, validate on 7989 samples\n",
      "Epoch 1/10\n",
      " - 456s - loss: 0.0593 - acc: 0.9789 - val_loss: 0.0465 - val_acc: 0.9820\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.982814 \n",
      "\n",
      "Epoch 2/10\n",
      " - 449s - loss: 0.0433 - acc: 0.9832 - val_loss: 0.0489 - val_acc: 0.9813\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986696 \n",
      "\n",
      "Epoch 3/10\n",
      " - 449s - loss: 0.0381 - acc: 0.9847 - val_loss: 0.0442 - val_acc: 0.9833\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.986360 \n",
      "\n",
      "Epoch 00003: early stopping\n",
      "153164/153164 [==============================] - 59s 387us/step\n",
      "Running fold 2 / 4\n",
      "Training / Valid set counts (151588,) / (7983,)\n",
      "Train on 151588 samples, validate on 7983 samples\n",
      "Epoch 1/10\n",
      " - 458s - loss: 0.0639 - acc: 0.9780 - val_loss: 0.0560 - val_acc: 0.9796\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.976736 \n",
      "\n",
      "Epoch 2/10\n",
      " - 454s - loss: 0.0432 - acc: 0.9831 - val_loss: 0.0523 - val_acc: 0.9804\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.981848 \n",
      "\n",
      "Epoch 3/10\n",
      " - 450s - loss: 0.0379 - acc: 0.9849 - val_loss: 0.0497 - val_acc: 0.9803\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.983088 \n",
      "\n",
      "Epoch 4/10\n",
      " - 449s - loss: 0.0339 - acc: 0.9862 - val_loss: 0.0509 - val_acc: 0.9797\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.982365 \n",
      "\n",
      "Epoch 00004: early stopping\n",
      "153164/153164 [==============================] - 59s 386us/step\n",
      "Running fold 3 / 4\n",
      "Training / Valid set counts (151584,) / (7987,)\n",
      "Train on 151584 samples, validate on 7987 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1258b64e1417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     history = model.fit(xs_train, ys_train, batch_size = batch_size, epochs = epochs, validation_data = (xs_valid, ys_valid), \n\u001b[0;32m---> 35\u001b[0;31m                           verbose = 2, callbacks=[RocAuc, csv_logger, CheckPoint])        \n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Stratified k-fold training\n",
    "\n",
    "n_folds = 4\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "predict_batch_size = 1024\n",
    "run_id = 'cnn_glove'\n",
    "opt = Nadam(lr=0.002) #optimizer\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = 20, shuffle = True, random_state = 32)\n",
    "\n",
    "csv_logger = CSVLogger('../training.log')\n",
    "early_stop = EarlyStopping(verbose=2)\n",
    "\n",
    "pred = np.zeros((x_test.shape[0], 6))\n",
    "y_packed = np.packbits(y_train, axis=1)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold.split(x_train, y_packed)):\n",
    "    print(\"Running fold {} / {}\".format(i + 1, n_folds))\n",
    "    print(\"Training / Valid set counts {} / {}\".format(train_idx.shape, valid_idx.shape))\n",
    "\n",
    "    model = None    \n",
    "    model = get_model()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    \n",
    "    xs_train, ys_train = x_train[train_idx], y_train[train_idx] \n",
    "    xs_valid, ys_valid = x_train[valid_idx], y_train[valid_idx]\n",
    "\n",
    "    CheckPoint = ModelCheckpoint('../snapshots/' + run_id + '_fold_' + str(i) + '_weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "    RocAuc = RocAucEvaluation(validation_data=(xs_valid, ys_valid), interval=1)\n",
    "\n",
    "    # training\n",
    "    history = model.fit(xs_train, ys_train, batch_size = batch_size, epochs = epochs, validation_data = (xs_valid, ys_valid), \n",
    "                          verbose = 2, callbacks=[RocAuc, csv_logger, CheckPoint])        \n",
    "    # predict\n",
    "    pred += model.predict(x_test, batch_size = predict_batch_size, verbose = 1)\n",
    "\n",
    "    if (i + 1) == n_folds: break    \n",
    "    \n",
    "y_pred = pred/n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = load_model('../snapshots/lstm_glove_fold_0_weights.02-0.04.hdf5')\n",
    "mod2 = load_model('../snapshots/lstm_glove_fold_1_weights.02-0.04.hdf5')\n",
    "\n",
    "pred1 = mod1.predict(x_test, batch_size = predict_batch_size, verbose = 1)\n",
    "pred2 = mod2.predict(x_test, batch_size = predict_batch_size, verbose = 1)\n",
    "\n",
    "y_pred1 = (pred1 + pred2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_glove_fold_0_weights.03-0.04.hdf5'\n",
    "model = load_model('../snapshots/' + model_name)\n",
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('../submissions/cnn_2_window_2_3_filter_64_l1_lstm80_spatial_dr_0_4_lstm_dr_0_2_dense_64_dr_0_2_glove_ep3_batch_128.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN (filters 64, window 2 (dropout 0.2) and window 3 (dropout 0.2) ) + LSTM (80, dropout=0.2, recurrent_dropout=0.2 - Spatial Dropout 0.4) + Glove + last dense 64 dropout (0.2) - Ep3 - Batch size 128, Max features 100,000\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/3\n",
    " - 902s - loss: 0.0565 - acc: 0.9801 - val_loss: 0.0450 - val_acc: 0.9818\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.986522 \n",
    "\n",
    "Epoch 2/3\n",
    " - 880s - loss: 0.0417 - acc: 0.9837 - val_loss: 0.0421 - val_acc: 0.9832\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989570 \n",
    "\n",
    "Epoch 3/3\n",
    " - 872s - loss: 0.0372 - acc: 0.9849 - val_loss: 0.0414 - val_acc: 0.9841\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.990200 \n",
    " \n",
    " * **LB 0.9830**\n",
    "\n",
    "\n",
    "#### CNN (filters 64, window 2 (dropout 0.2) and window 3 (dropout 0.2) ) + LSTM (80, dropout=0.2, recurrent_dropout=0.2 - Spatial Dropout 0.4) + Glove + last dense 64 dropout (0.1) - Ep3 - Batch size 128, Max features 100,000\n",
    "\n",
    "\n",
    "* Name: **cnn_2_window_2_3_filter_64_l1_lstm80_spatial_dr_0_4_lstm_dr_0_2_fasttext_ep3_batch_128.csv**\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/3\n",
    " - 881s - loss: 0.0577 - acc: 0.9798 - val_loss: 0.0449 - val_acc: 0.9822\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.984894 \n",
    "\n",
    "Epoch 2/3\n",
    " - 875s - loss: 0.0413 - acc: 0.9839 - val_loss: 0.0415 - val_acc: 0.9837\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989601 \n",
    "\n",
    "Epoch 3/3\n",
    " - 870s - loss: 0.0365 - acc: 0.9854 - val_loss: 0.0406 - val_acc: 0.9839\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.990294 \n",
    "\n",
    "* **LB 0.9833**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### CNN (filters 64, window 2 (dropout 0.2) and window 3 (dropout 0.2) ) + LSTM (64, dropout=0.2, recurrent_dropout=0.2 - Spatial Dropout 0.4) + FastText + MaxPool - Ep2 - Batch size 128, Max features 100,000\n",
    "\n",
    "* Total params: 30,332,486\n",
    "\n",
    "**Attempt 1**\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/2\n",
    " - 876s - loss: 0.0325 - acc: 0.9866 - val_loss: 0.0430 - val_acc: 0.9837\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.989429 \n",
    "\n",
    "Epoch 2/2\n",
    " - 877s - loss: 0.0292 - acc: 0.9879 - val_loss: 0.0446 - val_acc: 0.9836\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989135 \n",
    " \n",
    " \n",
    " **Attempt 2**\n",
    " \n",
    " * name: **cnn_2_window_2_3_l1_lstm256_spatial_dr_0_4_lstm_dr_0_2_fasttext_maxpool_ep2_batch_128.csv**\n",
    " \n",
    " Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/2\n",
    " - 887s - loss: 0.0552 - acc: 0.9803 - val_loss: 0.0445 - val_acc: 0.9821\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.987122 \n",
    "\n",
    "Epoch 2/2\n",
    " - 868s - loss: 0.0409 - acc: 0.9840 - val_loss: 0.0413 - val_acc: 0.9837\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989921 \n",
    "\n",
    "\n",
    "#### CNN (filters 64, window 2 and window 3) + LSTM (256, dropout=0.5, recurrent_dropout=0.5 - Spatial Dropout 0.4) + FastText + MaxPool - Ep3 - Batch size 256, Max features 100,000\n",
    "\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/3\n",
    " - 655s - loss: 0.0576 - acc: 0.9794 - val_loss: 0.0547 - val_acc: 0.9788\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.988215 \n",
    "\n",
    "Epoch 2/3\n",
    " - 623s - loss: 0.0427 - acc: 0.9837 - val_loss: 0.0498 - val_acc: 0.9813\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.988926 \n",
    "\n",
    "Epoch 3/3\n",
    " - 624s - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0546 - val_acc: 0.9815\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.988650 \n",
    "\n",
    "**Attempt 2**\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/3\n",
    " - 627s - loss: 0.0346 - acc: 0.9863 - val_loss: 0.0566 - val_acc: 0.9806\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.988491 \n",
    "\n",
    "Epoch 2/3\n",
    " - 628s - loss: 0.0311 - acc: 0.9877 - val_loss: 0.0479 - val_acc: 0.9818\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.987230 \n",
    "\n",
    "Epoch 3/3\n",
    " - 629s - loss: 0.0283 - acc: 0.9888 - val_loss: 0.0552 - val_acc: 0.9801\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.986574 \n",
    "\n",
    "**Attempt 3**\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/3\n",
    " - 639s - loss: 0.0572 - acc: 0.9794 - val_loss: 0.0913 - val_acc: 0.9669\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.983046 \n",
    "\n",
    "Epoch 2/3\n",
    " - 635s - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0500 - val_acc: 0.9802\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989757 \n",
    "\n",
    "Epoch 3/3\n",
    " - 637s - loss: 0.0361 - acc: 0.9857 - val_loss: 0.0575 - val_acc: 0.9774\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.988192 \n",
    "\n",
    "\n",
    "**Attempt 4**\n",
    "\n",
    "* Total params: 31,312,390\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/2\n",
    " - 643s - loss: 0.0571 - acc: 0.9797 - val_loss: 0.0528 - val_acc: 0.9790\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.988197 \n",
    "\n",
    "Epoch 2/2\n",
    " - 640s - loss: 0.0411 - acc: 0.9841 - val_loss: 0.0443 - val_acc: 0.9831\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.989429 \n",
    "\n",
    "**LB 0.9837**\n",
    "\n",
    "\n",
    "#### CNN (filters 128, window 2) + GRU (256) + FastText + MaxPool - Ep3 - dropout=0.5, recurrent_dropout=0.5 - Spatial Dropout 0.4 - Batch size 128, Max features 100,000\n",
    "\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/5\n",
    " - 724s - loss: 0.0551 - acc: 0.9803 - val_loss: 0.0943 - val_acc: 0.9755\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.981225 \n",
    "\n",
    "Epoch 2/5\n",
    " - 722s - loss: 0.0431 - acc: 0.9835 - val_loss: 0.0731 - val_acc: 0.9794\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.983226 \n",
    "\n",
    "Epoch 3/5\n",
    " - 722s - loss: 0.0381 - acc: 0.9851 - val_loss: 0.0652 - val_acc: 0.9781\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.985856 \n",
    "\n",
    "Epoch 4/5\n",
    " - 722s - loss: 0.0438 - acc: 0.9848 - val_loss: 0.0803 - val_acc: 0.9714\n",
    "\n",
    " ROC-AUC - epoch: 4 - score: 0.973663 \n",
    "\n",
    "Epoch 5/5\n",
    "\n",
    "#### CNN (filters 64, window 2) + GRU (256) + FastText + MaxPool - Ep3 - dropout=0.5, recurrent_dropout=0.5 - Spatial Dropout 0.4 - Batch size 128, Max features 100,000\n",
    "\n",
    "Train on 151592 samples, validate on 7979 samples\n",
    "Epoch 1/5\n",
    " - 717s - loss: 0.0550 - acc: 0.9802 - val_loss: 0.0644 - val_acc: 0.9816\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.987303 \n",
    "\n",
    "Epoch 2/5\n",
    " - 714s - loss: 0.0422 - acc: 0.9837 - val_loss: 0.0643 - val_acc: 0.9816\n",
    "\n",
    " ROC-AUC - epoch: 2 - score: 0.973863 \n",
    "\n",
    "Epoch 3/5\n",
    " - 714s - loss: 0.0465 - acc: 0.9829 - val_loss: 0.1030 - val_acc: 0.9644\n",
    "\n",
    " ROC-AUC - epoch: 3 - score: 0.978108 \n",
    "\n",
    "Epoch 4/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
