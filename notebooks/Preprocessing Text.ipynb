{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Dropout\n",
    "from keras.layers import GRU, LSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view\n",
    "# Aphost lookup dict\n",
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}\n",
    "\n",
    "# https://www.kaggle.com/prashantkikani/hight-of-preprocessing-with-emoji\n",
    "# https://www.kaggle.com/prashantkikani/pooled-gru-with-preprocessing\n",
    "\n",
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"its\" : \"it is\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "}\n",
    "\n",
    "replacements = {**APPO, **repl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function receives comments and returns clean word-list\n",
    "    \"\"\"\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "    comment=comment.lower()\n",
    "    #remove \\n\n",
    "    # comment = re.sub(\"\\\\n\",\"\", comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\" ip \", comment)\n",
    "    # replace words like Gooood with Good\n",
    "    comment = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', comment)\n",
    "    # replace ! and ? with <space>! and <space>? so they can be kept as tokens\n",
    "    comment = re.sub(r'(!|\\?)', \" \\\\1 \", comment)   \n",
    "        \n",
    "    #Split the sentences into words\n",
    "    words=comment.split(' ')\n",
    "    \n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "    words=[replacements[word] if word in replacements else word for word in words]\n",
    "    #words=[lem.lemmatize(word, pos=\"v\") for word in words]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    clean_sent=\" \".join(words)\n",
    "    # remove any non alphanum,digit character\n",
    "    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    return(clean_sent)\n",
    "\n",
    "def count_ips(comment): #10081, 756\n",
    "    return 1 if re.search(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', comment) else 0\n",
    "    \n",
    "def count_abbreviations(comment): #67144, 61826\n",
    "    return np.bitwise_or.reduce([1 if word in replacements else 0 for word in comment.lower().split(' ')], 0)\n",
    "\n",
    "j = 0\n",
    "def count_lemmatized(comment):  #138,570, 126,810\n",
    "    global j\n",
    "    \n",
    "    if j > 100: return 0\n",
    "    \n",
    "    words = comment.lower().split(' ')\n",
    "    changed = set([lem.lemmatize(word, pos=\"v\") for word in words]) - set(words)\n",
    "    if j < 20 and len(changed) > 0: print(changed, set(words) - set([lem.lemmatize(word, pos=\"v\") for word in words])); j = j + 1\n",
    "    return len(changed) > 0\n",
    "\n",
    "i = 0\n",
    "def count_goood(comment): #42345, 60569\n",
    "    global i\n",
    "    \n",
    "    #if i < 20 and re.search(r'(\\w)\\1{2,}', comment): i = i+1; print(comment)\n",
    "    return 1 if re.search(r'(\\w)\\1{2,}', comment) else 0\n",
    "\n",
    "k = 0\n",
    "def check_char_in_record(char, comment):\n",
    "    global k\n",
    "    \n",
    "    if k < 20 and char in comment: print(comment); k = k + 1\n",
    "    return 1 if char in comment else 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character ! appears: 22821\n"
     ]
    }
   ],
   "source": [
    "#train = pd.read_csv('../data/train.csv')\n",
    "#test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "#print(\"Ips count: {}\".format(sum(test[\"comment_text\"].apply(lambda x: count_ips(x)))))\n",
    "#print(\"Abbeviation count: {}\".format(sum(test[\"comment_text\"].apply(lambda x: count_abbreviations(x)))))\n",
    "#print(\"Lemmatize count: {}\".format(sum(test[\"comment_text\"].apply(lambda x: count_lemmatized(x)))))\n",
    "#print(\"Gooood count: {}\".format(sum(test[\"comment_text\"].apply(lambda x: count_goood(x)))))\n",
    "\n",
    "c = '!'\n",
    "print(\"Character {} appears: {}\".format(c, sum(train[\"comment_text\"].apply(lambda x: check_char_in_record(c, x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time till Cleaning 18.93160104751587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "train[\"comment_text\"] = train[\"comment_text\"].apply(lambda x: clean(x))\n",
    "test[\"comment_text\"] = test[\"comment_text\"].apply(lambda x: clean(x))\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"total time till Cleaning\", end_time-start_time)\n",
    "\n",
    "train.to_csv('../data/train_cleaned.csv', index=False)\n",
    "test.to_csv('../data/test_cleaned.csv', index=False)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ips count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"d'aww !  matches background colour i am seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Ips count: {}\".format(sum(train[\"comment_text\"].apply(lambda x: count_ips(x)))))\n",
    "#print(\"Abbeviation count: {}\".format(sum(train[\"comment_text\"].apply(lambda x: count_abbreviations(x)))))\n",
    "#print(\"Lemmatize count: {}\".format(sum(train[\"comment_text\"].apply(lambda x: count_lemmatized(x)))))\n",
    "#print(\"Gooood count: {}\".format(sum(train[\"comment_text\"].apply(lambda x: count_goood(x)))))\n",
    "\n",
    "c = '!'\n",
    "print(\"Character {} appears: {}\".format(c, sum(train[\"comment_text\"].apply(lambda x: check_char_in_record(c, x)))))\n",
    "#train[\"comment_text\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequencing and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/glove/glove.840B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "embeddings = 'glove' #'glove', 'fasttext\n",
    "\n",
    "if embeddings == 'fasttext':\n",
    "    EMBEDDING_FILE = '../data/fasttext/crawl-300d-2M.vec' #'../data/fasttext/crawl-300d-2M.vec'\n",
    "else:\n",
    "    EMBEDDING_FILE = '../data/glove/glove.840B.300d.txt'    \n",
    "\n",
    "max_features = 100000  #100000 , 30000\n",
    "maxlen = 200\n",
    "embed_size = 300\n",
    "prefix = 'c1' #x, #c1\n",
    "\n",
    "print(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = 'train_cleaned.csv'\n",
    "test_file = 'test_cleaned.csv'\n",
    "\n",
    "#train_file = 'train.csv'\n",
    "#test_file = 'test.csv'\n",
    "\n",
    "train = pd.read_csv('../data/' + train_file)\n",
    "test = pd.read_csv('../data/' + test_file)\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features, filters='\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n',)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 392107)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features, len(word_index) #clean (100000, 392107) original (100000, 394787)\n",
    "#first2pairs = {k: word_index[k] for k in list(word_index)[:10]}\n",
    "#first2pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "index_file = 'word_index_cleaned' #word_index_cleaned, word_index\n",
    "\n",
    "#pickle.dump(word_index, open('../models/' + index_file, 'wb'))\n",
    "word_index_cleaned = pickle.load(open('../models/' + index_file, 'rb') )\n",
    "\n",
    "index_file = 'word_index' #word_index_cleaned, word_index\n",
    "\n",
    "#pickle.dump(word_index, open('../models/' + index_file, 'wb'))\n",
    "word_index = pickle.load(open('../models/' + index_file, 'rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2680\n",
      "3091 5771\n",
      "1860166\n",
      "ooioioiknkjbjmb\n",
      "nomdeplume600\n",
      "f60e17fb345f12728ddab0a94d1405b8084f1d3\n",
      "mn008122\n",
      "frigilmm…\n",
      "esherraann\n",
      "iogaukrhjkwerhfrhrfakafyhurkruyiureuierrueirhrcgghhbhghhjdhguhgjhuhtghjhuhjhguhhgduthhghhgjhgk\n",
      "ffuucckkyyoouu\n",
      "francs200\n",
      "buenoo\n",
      "coo0ntface\n",
      "kxxr\n",
      "54005080010160020320040640081280162560325120650241300482600965201930403860807721615443230886461772923545847091694183388366776733553467106934213868427736855473710947421894843789687579375158750317500635001270025400\n",
      "iiff\n",
      "sadgg\n",
      "blahhblahhblahblahblah\n",
      "220417\n",
      "ncc'\n",
      "grrowlss\n",
      "3326764\n",
      "anubisii\n",
      "tomica11\n",
      "gb008237132gbgbxaimi\n",
      "annies\n",
      "jeangabin66\n",
      "steven88\n",
      "aajyfvlopugm\n",
      "jjakegittes\n",
      "rajukc11\n",
      "danlt88\n",
      "4faaibaj\n",
      "ghdfjghfjghfjghfjgfhgjkkfhfdjghdfgfhhggdjdjdsgdjhdgfhdgdhgehdydhdhdgdfdhsywshasudydfbdfydfdfh\n",
      "zicfaaibaj\n",
      "mbrr\n",
      "satann\n",
      "texdsgdfgdfgdhgfddghgdgfdgfteretrerterrittcxczxcfhkljdhgvcmjh\n",
      "aahhtchh\n",
      "37746428\n",
      "pmh001734\n",
      "helloog\n",
      "üğökppşppçöpp\n",
      "fsddfssdtrdttghghghghggttkkddxxvv\n",
      "mm'\n",
      "cleeaan\n",
      "bdell55\n",
      "wheellss\n",
      "bfogbstqetfsdfaefqff\n",
      "mommaa\n",
      "dipstick200\n",
      "mmbeer\n",
      "1оо\n",
      "d3ee2f074095882088de64e4d483812\n",
      "crandallab\n",
      "oggfy\n",
      "foreevver\n",
      "hjdifmhjj\n",
      "fbobooboboboboboboboboboboobooboboobooboobooboobooboobovfvb43f\n",
      "h14e455tr5y5yh5yy565t5jkr45tj55r5tkm52t25iot2yh25e5kij5r5t54l5y2oy66k656i5665i556\n",
      "bywaa\n",
      "86300\n",
      "shittysmitty44r\n",
      "?\n",
      "sander77\n",
      "dffdf\n",
      "1lzuaamaaj\n",
      "xxeminemxxeminemxxeminemxxeminemxxeminemxx\n",
      "201311\n",
      "waahh\n",
      "lloovve\n",
      "aarrgg\n",
      "100598720561\n",
      "19099676\n",
      "pgagnon99\n",
      "2093229\n",
      "200gt\n",
      "uuppd\n",
      "55put\n",
      "11211\n",
      "veryy\n",
      "arrhh\n",
      "mwuahh\n",
      "jeff300's\n",
      "yoouu\n",
      "aaoohh\n",
      "ffbc4\n",
      "trrhurtkjhguithtrgyrtghuirjhfuighurthughrihgwohgubvkdshdsuiygsserhurefgbdfhgdcbhgfgkserkarhglrtyjhpteujutghehuhghrtghihhuyfutfitguguffcddjgukghjhkiuihgyftdtdxfggjhkhluijikkuihjhuyhntgcghyhrtghiedgvjfhgfhydvrfvbdnfvhdgyuerbhfdyfjtgfdhbjgfuyrjhsdfuydfghdsjjfjhtftvfghrythtfgytvddghggtyfryhjfderdtyyjfyfdgdfjgjhdfgtsggygbyguyggyggygygjghghgjggjgjgjgtygtfdrdyuouhhk\n",
      "askari22\n",
      "immhoorhnunuss\n",
      "fukingg\n",
      "ax0daaibaj\n",
      "listiia\n",
      "iflnmlekhriubyiutypiuvoy5t43nuvy4oiutoiu4oicjuutoivpyucpjwoit2ruevoiutyreiuytiurtuewtoimcnvhgvvbcnxbcxnzzxcvbnmasdfghjklqwertyuioopqoiuc\n",
      "hfsqaaiaaj\n",
      "yeey\n",
      "bbastard\n",
      "slayer88's\n",
      "wm200\n",
      "barakatjune200\n",
      "11oneoneoneonoene\n",
      "deekk\n"
     ]
    }
   ],
   "source": [
    "#len(word_index), len(word_index_cleaned)\n",
    "\n",
    "print(len(word_index_cleaned.keys()) - len(word_index.keys()))\n",
    "#diff1 = set(word_index_cleaned.keys()) - set(word_index.keys())\n",
    "#diff2 = set(word_index.keys()) - set(word_index_cleaned.keys())\n",
    "\n",
    "print(len(diff1), len(diff2))\n",
    "#it = iter(diff1)\n",
    "#next(it)\n",
    "\n",
    "for i, e in enumerate(diff1):\n",
    "    print(e)\n",
    "    if i == 100: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index_cleaned['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_feats_path = '../models/{}_train_feat_{}_seq_{}.pkl'.format(prefix, max_features, maxlen)\n",
    "test_feats_path = '../models/{}_test_feat_{}_seq_{}.pkl'.format(prefix, max_features, maxlen)\n",
    "embedding_matrix_path = '../models/{}_{}_embedding_matrix_feat_{}.pkl'.format(prefix, embeddings, max_features)\n",
    "\n",
    "pickle.dump(x_train, open(train_feats_path, 'wb'))\n",
    "pickle.dump(x_test, open(test_feats_path, 'wb'))\n",
    "pickle.dump(embedding_matrix, open(embedding_matrix_path, 'wb'))\n",
    "\n",
    "#x_train = pickle.load(open(train_feats_path, 'rb') )\n",
    "#x_test = pickle.load(open(test_feats_path, 'rb') )\n",
    "#embedding_matrix = pickle.load(open(embedding_matrix_path, 'rb') )\n",
    "\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparing\n",
    "prefix = 'x' #x, #c1\n",
    "\n",
    "train_feats_path_2 = '../models/{}_train_feat_{}_seq_{}.pkl'.format(prefix, max_features, maxlen)\n",
    "test_feats_path_2 = '../models/{}_test_feat_{}_seq_{}.pkl'.format(prefix, max_features, maxlen)\n",
    "embedding_matrix_path_2 = '../models/{}_{}_embedding_matrix_feat_{}.pkl'.format(prefix, embeddings, max_features)\n",
    "\n",
    "x_train_2 = pickle.load(open(train_feats_path_2, 'rb') )\n",
    "x_test_2 = pickle.load(open(test_feats_path_2, 'rb') )\n",
    "embedding_matrix_2 = pickle.load(open(embedding_matrix_path_2, 'rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge=pd.concat([X_train,test.iloc[:,0:2]])\n",
    "#df=merge.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "sum(y_train[:,0]), sum(y_train[:,0])/len(y_train[:,0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train[y_train[:,0] == 1,:]\n",
    "y_packed = np.packbits(y_train, axis=1)\n",
    "len(y), len(y_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_packed[y_packed != 0])\n",
    "# sum(y_train[:,0] | y_train[:,1]  | y_train[:,2]  | y_train[:,3]  | y_train[:,4]  | y_train[:,5])\n",
    "#np.unpackbits(y_packed, axis=1)[:,0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 32)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold.split(X_train, y_packed)):\n",
    "    if i > n_folds: break\n",
    "\n",
    "    print(i, train_idx.shape)\n",
    "    print(\"Running fold {} / {}\".format(i + 1, n_folds))\n",
    "    \n",
    "    model = build_model(units = units, dr = dr, lr_i = lr_i, lr_f = lr_f, batch_size = batch_sizes, epoch = epochs)\n",
    "\n",
    "    x_train, y_train = X_train[train_idx], Y_train[train_idx] \n",
    "    x_valid, y_valid = X_train[valid_idx], Y_train[valid_idx]\n",
    "\n",
    "    history = model.fit(x_train, y_train, batch_size = batch_sizes, epochs = epochs, validation_data = (x_valid, y_valid), \n",
    "                          verbose = 2, callbacks = [check_point, early_stop, ra_val])\n",
    "    model = load_model(file_path)\n",
    "    pred += model.predict(X_test, batch_size = batch_sizes, verbose = 1)\n",
    "    \n",
    "preds = pred/n_folds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* Include ! \n",
    "* Goooood -> Good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful features\n",
    "\n",
    "* Repeated char acounts (!$?)\n",
    "* All capital"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
